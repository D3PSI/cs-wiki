# Sequential Logic

# Introduction

The outputs of *sequential* logic depend on both current and prior input values. Hence, sequential logic has memory. Sequential logic might explicitly remember certain previous inputs, or it might distill the prior inputs into a smaller amount of information called the *state* of the system. The state of a digital sequential circuit is a set of bits called *state variables* that contain all the information about the past necessary to explain the future behavior of the circuit.

# Latches and Flip-Flops

The fundamental building block of memory is a *bistable* element, an element with two stable states. The figure shows a simple bistable element consisting of a pair of inverters connected in a loop. The same circuit is redrawn to emphasize the symmetry. The inverters are *cross-coupled*, meaning that the input of I1 is the output of I2 and vice versa. The circuit has no inputs, but it does have two outputs, $Q$ and $\bar{Q}$. Analyzing this circuit is different from analyzing a combinational circuit because it is cyclic: $Q$ depends on $\bar{Q}$, and $\bar{Q}$ depends on $Q$.

![Untitled](Sequential%20Logic%2042c224c8465c4576ac93922cb2672120/Untitled.png)

Consider the two cases, $Q$ is 0 or $Q$ is 1. For each case the following consequences result:

- Case 1: $Q = 0$
    
    As shown in figure a), I2 receives a FALSE input, $Q$, so it produces a TRUE output on $\bar{Q}$. I1 receives a TRUE input, $\bar{Q}$, so it produces a FALSE output on $Q$. This is consistent with the original assumption that $Q = 0$, so the case is said to be *stable*.
    
- Case 2: $Q = 1$
    
    As shown in figure b), I2 receives a TRUE input and produces a FALSE output on $\bar{Q}$. I1 receives a FALSE input and produces a TRUE output on $Q$. This is again stable.
    

![Untitled](Sequential%20Logic%2042c224c8465c4576ac93922cb2672120/Untitled%201.png)

Because the cross-coupled inverters have two stable states, $Q = 0$ and $Q = 1$, the circuit is said to be bistable. A subtle point is that the circuit has a third possible state with both outputs approximately halfway between 0 and 1. This is called a *metastable state* and will be discussed later.

An element with $N$ stable states conveys $\log_2 N$ bits of information, so a bistable element stores one bit. The state of the cross-coupled inverters is contained in one binary state variable, $Q$. The value of $Q$ tells us everything about the past that is necessary to explain the future behavior of the circuit. Specifically, if $Q = 0$, it will remain 0 forever, and if $Q = 1$, it will remain 1 forever. The circuit does have another node, $\bar{Q}$, but $\bar{Q}$ does not contain any additional information because if $Q$ is known, then $\bar{Q}$ is also known. On the other hand, $\bar{Q}$ is also an acceptable choice for the state variable.

When power is first applied to a sequential circuit, the initial state is unknown and usually unpredictable. it may differ each time the circuit is turned on.

Although the cross-coupled inverters can store a bit of information, they are not practical because the user has no inputs to control the state. However, other bistable elements, such as *latches* and *flip-flops*, provide inputs to control the value of the state variable.

## SR Latch

One of the simplest sequential circuits is the *SR latch*, which is composed of two cross-coupled $\text{NOR}$ gates as shown. The latch has two inputs, $S$ and $R$, and two outputs, $Q$ and $\bar{Q}$. The SR latch is similar to the cross-coupled inverters, but its state can be controlled through the $S$ and $R$ inputs, which *set* and *reset* the output $Q$.

A good way to understand an unfamiliar circuit is to work out its truth table, so let’s start there. Recall that a $\text{NOR}$ gate produces a FALSE output when either input is TRUE. Consider the four possible combinations of $R$ and $S$:

- Case 1: $R = 1, S = 0$
    
    N1 sees at least one TRUE input, $R$, so it produces a FALSE output on $Q$. N2 sees both $Q$ and $S$ FALSE, so it produces a TRUE output on $\bar{Q}$.
    
- Case 2: $R = 0, S = 1$
    
    N1 receives inputs of 0 and $\bar{Q}$. Because we don’t yet know $\bar{Q}$, we can’t determine the output $Q$. N2 receives at least one TRUE input, $S$, so it produces a FALSE output on $\bar{Q}$. Now we can revisit N1, knowing that both inputs are FALSE, so the output $Q$ is TRUE.
    
- Case 3: $R = 1, S = 1$
    
    N1 and N2 both see at least one TRUE input ($R$ or $S$), so each produces a FALSE output. Hence $Q$ and $\bar{Q}$ are both FALSE.
    
- Case 4: $R = 0, S = 0$
    
    N1 receives input of 0 and $\bar{Q}$. Because we don’t yet know $\bar{Q}$, we can’t determine the output. N2 receives inputs of 0 and $Q$. Because we don’t yet know $Q$, we can’t determine the output. Now we are stuck. This is reminiscent of the cross-coupled inverters. But we know that $Q$ must either be 0 or 1. So we should be able to solve the problem by checking what happens in each of these subcases.
    
    - Case 4a: $Q = 0$
        
        Because $S$ and $Q$ are FALSE, N2 produces a TRUE output on $\bar{Q}$, as shown in a). Now N1 receives one TRUE input, $\bar{Q}$, so its output, $Q$, is FALSE, just as we had assumed.
        
    - Case 4b: $Q = 1$
        
        Because $Q$ is TRUE, N2 produces a FALSE output on $\bar{Q}$, as shown in b). Now N1 receives two FALSE inputs, $R$ and $\bar{Q}$, so its output, $Q$, is TRUE, just as we had assumed.
        

![Untitled](Sequential%20Logic%2042c224c8465c4576ac93922cb2672120/Untitled%202.png)

![Untitled](Sequential%20Logic%2042c224c8465c4576ac93922cb2672120/Untitled%203.png)

Putting this all together, suppose $Q$ has some known prior value, which we shall call $Q_{\text{prev}}$, before we enter Case 4. $Q_{\text{prev}}$ is either 0 or 1, and represents the state of the system. When $R$ and $S$ are 0, $Q$ will remember this old value, $Q_{\text{prev}}$ and $\bar{Q}$ will be its complement, $\bar{Q}_{\text{prev}}$. The circuit has memory.

The truth table shown summarizes the four cases. The inputs $S$ and $R$ stand for *Set* and *Reset*. To *set* a bit means to make it TRUE. To *reset* a bit means to make it FALSE. The outputs, $Q$ and $\bar{Q}$, are normally complementary. When $R$ is asserted, $Q$ is reset to 0 and $\bar{Q}$ does the opposite. When $S$ is asserted, $Q$ is set to 1 and $\bar{Q}$ does the opposite. When neither input is asserted, $Q$ remembers its old value, $Q_{\text{prev}}$. Asserting both $S$and $R$ simultaneously does not make much sense because it means the latch should be set and reset at the same time, which is kind of impossible. The poor circuit has no other choice but to make both outputs 0, which makes no sense.

![Untitled](Sequential%20Logic%2042c224c8465c4576ac93922cb2672120/Untitled%204.png)

The SR latch is represented by the symbol shown. Using the symbol is again an application of abstraction and modularity. There are various ways to build an SR latch, such as using different logic gates or transistors. Nevertheless, any circuit element with the relationship specified by the above truth table and the symbol shown is called an SR latch.

![Untitled](Sequential%20Logic%2042c224c8465c4576ac93922cb2672120/Untitled%205.png)

Like the cross-coupled inverters, the SR latch is a bistable element with one bit of state stored in $Q$. However, the state can be controlled through the $S$ and $R$ inputs.

## D Latch

The SR latch is awkward because it behaves strangely when both inputs are asserted simultaneously. Moreover, the inputs conflate the issues of *what* and *when*. Asserting one of the inputs determines not only *what* the state should be but also *when* it should change. Circuit design becomes a lot easier when these questions are separated. The D latch in the figure solves these issues. It has two inputs. The *data* input, $D$, controls what the next state should be. The *clock* input, $\text{CLK}$, controls when the state should change.

Again, we can analyze the latch by writing the truth table, given in the figure. We notice that in all cases, $\bar{Q}$ is the complement of $Q$, as would seem logical. The D latch avoids the strange case of simultaneously asserted $R$ and $S$ inputs.

Putting it all together, we see that the clock controls when data flows through the latch. When $\text{CLK} = 1$, the latch is *transparent*. The data at $D$ flows through to $Q$ as if the latch were just a buffer. When $\text{CLK} = 0$, the latch is *opaque*. It blocks the new data from flowing through to $Q$, and $Q$ retains the old value. Hence, the D latch is sometimes called a *transparent latch* or a *level-sensitive latch*. The D latch symbol is given in c).

The D latch updates its state continuously while $\text{CLK} = 1$.

![Untitled](Sequential%20Logic%2042c224c8465c4576ac93922cb2672120/Untitled%206.png)

## D Flip-Flop

A *D flip-flop* can be built from two back-to-back D latches controlled by complementary clocks, as shown. The first latch, L1, is called the *master*. The second latch, L2, is called the *slave*. The node between them is named N1. Symbols for the D flip-flop are given in b) and c).

When $\text{CLK} = 0$, the master latch is transparent and the slave is opaque. Therefore, whatever value was at $D$ propagates through to N1. When $\text{CLK} = 1$, the master goes opaque and the slave becomes transparent. The value at N1 propagates through to $Q$, but N1 is cut off from $D$. hence, whatever value was at $D$ immediately before the clock rises from 0 to 1 gets copied to $Q$ immediately after the clock rises. At all other times, $Q$ retains its old value, because there is always an opaque latch blocking the path between $D$ and $Q$.

In other words, *a $D$ flip-flop copies $D$ to $Q$ on the rising edge of the clock, and remembers its state at all other times*. The rising edge of the clock is often just called the *clock edge* for brevity.The $D$ input specifies what the new state will be. The clock edge indicates when the state should be updated.

A D flip-flop is also known as a *master-slave flip-flop*, an *edge-triggered flip-flop*, or a *positive edge-triggered flip-flop*. The triangle in the symbols denotes an edge-triggered clock input.

![Untitled](Sequential%20Logic%2042c224c8465c4576ac93922cb2672120/Untitled%207.png)

## Register

An $N$-bit register is a bank of $N$ flip-flops that share a common $\text{CLK}$ input, so that all bits of the register are updated at the same time. Registers are the key building block of most sequential circuits. The figure shows the schematic and symbol for a four-bit register with inputs $D_{3:0}$ and outputs $Q_{3:0}$. $D_{3:0}$ and $Q_{3:0}$ are both 4-bit busses.

![Untitled](Sequential%20Logic%2042c224c8465c4576ac93922cb2672120/Untitled%208.png)

## Enabled Flip-Flop

An *enabled flip-flop* adds another input called $\text{EN}$ or ENABLE to determine whether data is loaded on the clock edge. When $\text{EN}$ is TRUE, the enabled flip-flop behaves like an ordinary D flip-flop. When $\text{EN}$ is FALSE, the enabled flip-flop ignores the clock and retains its state. Enabled flip-flops are useful when we wish to load a new value into the flip-flop only some of the time, rather than on every clock edge.

The figure shown two ways to construct an enabled flip-flop from a D flip-flop and an extra gate. In the first figure, an input multiplexer chooses whether to pass the value at $D$, if $\text{EN}$ is TRUE, or to recycle the old state from $Q$, if $\text{EN}$ is FALSE. In the second figure, the clock is *gated*. If $\text{EN}$ is TRUE, the $\text{CLK}$ input to the flip-flop toggles normally. If $\text{EN}$ is FALSE, the $\text{CLK}$ input is also FALSE and the flip-flop retains its old value. Notice that $\text{EN}$ must not change while $\text{CLK} = 1$, lest the flip-flop see a clock *glitch*. Generally, performing logic on the clock is a bad idea. Clock gating delays the clock and can cause timing errors, as we will see later.

![Untitled](Sequential%20Logic%2042c224c8465c4576ac93922cb2672120/Untitled%209.png)

## Resettable Flip-Flop

A *resettable flip-flop* adds another input called $\text{RESET}$. When $\text{RESET}$ is FALSE, the resettable flip-flop behaves like an ordinary D flip-flop. When $\text{RESET}$ is TRUE, the resettable flip-flop ignores $D$ and resets the output to 0. Resettable flip-flops are useful when we want to force a known state into all the flip-flops in a system when we first turn it on.

Such flip-flops may be *synchronously* or *asynchronously resettable*. Synchronously resettable flip-flops reset themselves only on the rising edge of the clock. Asynchronously resettable flip-flops reset themselves as soon as $\text{RESET}$ becomes TRUE, independent of the clock.

The figure shows how to construct a synchronously resettable flip-flop from an ordinary D flip-flop and an $\text{AND}$ gate. 

Asynchronously resettable flip-flops require modifying the internal structure of the flip-flop.

As one might imagine, settable flip-flops are also occasionally used. They load a 1 into the flip-flop when $\text{SET}$ is asserted, and they too come in synchronous and asynchronous flavors. Resettable and settable flip-flops may also have an enable input and may be grouped into $N$-bit registers.

![Untitled](Sequential%20Logic%2042c224c8465c4576ac93922cb2672120/Untitled%2010.png)

## Transistor-Level Latch and Flip-Flop Designs

Latches and flip-flops require a large number of transistors when built from logic gates. But the fundamental role of a latch is to be transparent or opaque, much like a switch. Recall from [Introduction](Introduction%201ba015f9cd614446bdb4213e70a154d9.md) that a transmission gate is an efficient way to build a CMOS switch, so we might expect that we could take advantage of transmission gates to reduce the transistor count. 

A compact D latch can be constructed from a single transmission gate, as shown. 

This latch suffers from two major limitations:

- *floating output node*: when the latch is opaque, $Q$ is not held at its value by any gates. This $Q$ is called a *floating* or *dynamic* node. After some time, noise and charge leakage may disturb the value of $A$.
- *no buffers*: the lack of buffers has caused malfunctions on several commercial chips. A spike of noise that pulls $D$ to a negative voltage can turn on the nMOS transistor, making the latch transparent, even when $\text{CLK} = 0$. Likewise, a spike on $D$ above $V_{DD}$ can turn on the pMOS transistor even when $\text{CLK} = 0$. And the transmission gate is symmetric, so it could be driven backward with noise on $Q$ affecting the input $D$. The general rule is that neither the input of a transmission gate nor the state node of a sequential circuit should ever be exposed to the outside world, where noise is likely.

![Untitled](Sequential%20Logic%2042c224c8465c4576ac93922cb2672120/Untitled%2011.png)

The figure shows a more robust 12-transistor D latch used on modern commercial chips. It is still built around a clocked transmission gate, but it adds inverters I1 and I2 to buffer the input and output. The state of the latch is held on node N1. Inverter I3 and the tristate buffer T1 provide feedback to turn N1 into a *static node*. If a small amount of noise occurs on N1 while $\text{CLK} = 0$, T1 will drive N1 back to a valid logic value.

![Untitled](Sequential%20Logic%2042c224c8465c4576ac93922cb2672120/Untitled%2012.png)

The figure shows a D flip-flop constructed from two static latches controlled by $\overline{\text{CLK}}$ and $\text{CLK}$. Some redundant internal inverters have been removed, so the flip-flop requires only 20 transistors. 

![Untitled](Sequential%20Logic%2042c224c8465c4576ac93922cb2672120/Untitled%2013.png)

# Synchronous Logic Design

In general, sequential circuits include all circuits that are not combinational - that is, those whose output cannot be determined by simply looking at the current inputs.

## Unstable Circuits

This circuit, where three inverters are routed in series, feeding the output back as the input, is an example of an unstable circuit. 

Assume node $X$ is 0 initially. Then $Y = 1$, $Z = 0$ and hence $X = 1$, which is inconsistent with out initial assumption. The circuit has no stable state and is said to be *unstable* or *astable*.

![Untitled](Sequential%20Logic%2042c224c8465c4576ac93922cb2672120/Untitled%2014.png)

Assume each inverter has a propagation delay of 1 ns. Then the timing diagram of the circuit looks as shown. Each node oscillates between 0 and 1 with a *period* of 6 ns. This circuit is called a *ring oscillator*.

![Untitled](Sequential%20Logic%2042c224c8465c4576ac93922cb2672120/Untitled%2015.png)

## Race Conditions

Let’s examine a new D latch design with fewer gates. 

![Untitled](Sequential%20Logic%2042c224c8465c4576ac93922cb2672120/Untitled%2016.png)

This circuit has a *race condition*, as can be seen from the timing diagram shown. This race condition causes it to fail when certain gates are slower than others. Suppose $\text{CLK} = D = 1$. The latch is transparent and passes $D$ through to make $Q = 1$. Now, $\text{CLK}$ falls. The latch should remember its old value, keeping $Q = 1$. However, suppose the delay through the inverter from $\text{CLK}$ to $\overline{\text{CLK}}$ is rather long compared to the delays of the $\text{AND}$ and $\text{OR}$ gates. Then nodes N1 and $Q$ may both fall before $\overline{\text{CLK}}$ rises. In such a case, N2 will never rise, and $Q$ becomes stuck at 0.

This is an example of an *asynchronous* circuit design in which outputs are directly fed back to inputs. Asynchronous circuits are infamous for having race conditions where the behavior of the circuit depends on which of two paths through logic gates is fastest. One circuit may work, while a seemingly identical one built from gates with slightly different delays may not work. Or the circuit may work only at certain temperatures or voltages at which the delays are just right. These mistakes are extremely difficult to track down.

![Untitled](Sequential%20Logic%2042c224c8465c4576ac93922cb2672120/Untitled%2017.png)

## Synchronous Sequential Circuits

The previous two examples contain loops called *cyclic paths*, in which outputs are fed directly back to inputs. They are sequential rather than combinational circuits. Combinational logic has no cyclic paths and no races. If inputs are applied to combinational logic, the outputs will always settle to the correct value within a propagation delay. However, sequential circuits with cyclic paths can have undesirable races or unstable behavior.

To avoid these problems, designers break the cyclic paths by inserting registers somewhere in the path, This transforms the circuit into a collection of combinational logic and registers. The registers contain the state of the system, which changes only at the clock edge, so we say the state is *synchronized* to the clock. If the clock is sufficiently slow, so that the inputs to all registers settle before the next clock edge, all races are eliminated. Adopting this discipline of always using registers in the feedback path leads us to the formal definition of a synchronous sequential circuit.

Recall that a circuit is defined by its input and output terminals and its functional and timing specifications. A sequential circuit has a finite set of discrete *states* $\{S_0, S_1,...,S_{k-1}\}$. A *synchronous sequential circuit* has a clock input, whose rising edges indicate a sequence of times at which state transitions occur. We often use the terms *current state* and *next state* to distinguish the state of the system at the present from the state to which it will enter on the next clock edge. The functional specification details the next state and the value of each output for each possible combination of current state and input values. The timing specification consists of an upper bound, $t_{pcq}$, and a lower bound, $t_{ccq}$, on the time from the rising edge of the clock until the *output* changes, as well as *setup* and *hold* times, 

# Finite State Machines

# Timing of Sequential Logic

# Parallelism